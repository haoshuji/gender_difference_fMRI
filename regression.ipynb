{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, Normalizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import Ridge, BayesianRidge, LinearRegression\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# linear_model.Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_names = [\"Ridge\",\n",
    "             \"BayesianRidge\",\n",
    "             \"LinearRegression\"\n",
    "            ]\n",
    "classifiers = [Ridge(),\n",
    "               BayesianRidge(),\n",
    "               LinearRegression()\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_features(clf, top_k=10, feature_names = []):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "#     feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        print clf.coef_\n",
    "        top_features_idx = np.argsort(clf.coef_[i])[-top_k:]\n",
    "        if feature_names == []:\n",
    "            print(\"{}:{}\".format(class_label, top_features_idx))\n",
    "        else:\n",
    "            print(\"%s: %s\" % (class_label,\n",
    "                  \" \".join(feature_names[j] for j in top_features_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_important_features(features, clf, feature_name_list=[], num_selected_features=50):\n",
    "    importances = clf.feature_importances_\n",
    "    X = np.array(features)\n",
    "    std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(0,num_selected_features):\n",
    "        if feature_name_list == []:\n",
    "            print(\"%d, (%f)\" % (f + 1, importances[indices[f]]))\n",
    "        else:\n",
    "            print(\"%d. feature %s (%f)\" % (f + 1, feature_name_list[indices[f]], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(num_selected_features), importances[indices[:num_selected_features]],\n",
    "           color=\"r\", yerr=std[indices[:num_selected_features]], align=\"center\")\n",
    "    plt.xticks(range(num_selected_features), indices[:num_selected_features])\n",
    "    plt.xlim([-1, num_selected_features])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiments(X, Y, feature_name_list=[], top_k=50):\n",
    "    # iterate over classifiers\n",
    "    '''\n",
    "    print('Accuracy of different classifier, without Normalization')\n",
    "    for name, clf in zip(clf_names, classifiers):\n",
    "        scores = cross_val_score(clf, X, Y, cv=5)\n",
    "        print(\"{}, {}/{}\".format(name, np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    adaboost_clf = AdaBoostClassifier()\n",
    "    adaboost_clf.fit(X, Y)\n",
    "    plot_important_features(X, adaboost_clf, feature_name_list, 50)\n",
    "    '''\n",
    "    \n",
    "    print('Accuracy of different classifier, with Normalization')\n",
    "    normalization_methods_name = ['StandardScaler', 'Normalizer']#, 'RobustScaler'\n",
    "    normalization_methods = [StandardScaler(), Normalizer()]#, RobustScaler(),\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(clf_names, classifiers):\n",
    "        print('\\n')\n",
    "        for norm_name, norm in zip(normalization_methods_name, normalization_methods):\n",
    "            loo = cross_validation.KFold(5)\n",
    "            scores = cross_validation.cross_val_score(make_pipeline(norm, clf), X, Y, scoring='neg_mean_squared_error', cv=loo)\n",
    "            clf.fit(X,Y)\n",
    "            print('{},{},mean_squared_error:{}'.format(name, norm_name,mean_squared_error(Y, clf.predict(X))))\n",
    "#             print(\"{},{}, {}/{}\".format(name, norm_name, np.mean(scores), np.std(scores)))\n",
    "#             print clf.coef_\n",
    "#     plot_important_features(X_scaled, clf, feature_name_list, top_k)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data into libsvm format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "\n",
      "Data:network_language\n",
      "\n",
      "Accuracy of different classifier, with Normalization\n",
      "\n",
      "\n",
      "Ridge,StandardScaler,mean_squared_error:43.3061959802\n",
      "Ridge,Normalizer,mean_squared_error:43.3061959802\n",
      "\n",
      "\n",
      "BayesianRidge,StandardScaler,mean_squared_error:43.5859108922\n",
      "BayesianRidge,Normalizer,mean_squared_error:43.5859108922\n",
      "\n",
      "\n",
      "LinearRegression,StandardScaler,mean_squared_error:39.0053979308\n",
      "LinearRegression,Normalizer,mean_squared_error:39.0053979308\n",
      "###############################################\n",
      "\n",
      "Data:network_perIQ\n",
      "\n",
      "Accuracy of different classifier, with Normalization\n",
      "\n",
      "\n",
      "Ridge,StandardScaler,mean_squared_error:77.2349732587\n",
      "Ridge,Normalizer,mean_squared_error:77.2349732587\n",
      "\n",
      "\n",
      "BayesianRidge,StandardScaler,mean_squared_error:77.4652875978\n",
      "BayesianRidge,Normalizer,mean_squared_error:77.4652875978\n",
      "\n",
      "\n",
      "LinearRegression,StandardScaler,mean_squared_error:66.0398924589\n",
      "LinearRegression,Normalizer,mean_squared_error:66.0398924589\n",
      "###############################################\n",
      "\n",
      "Data:network_spatial\n",
      "\n",
      "Accuracy of different classifier, with Normalization\n",
      "\n",
      "\n",
      "Ridge,StandardScaler,mean_squared_error:71.7351638445\n",
      "Ridge,Normalizer,mean_squared_error:71.7351638445\n",
      "\n",
      "\n",
      "BayesianRidge,StandardScaler,mean_squared_error:69.3561811831\n",
      "BayesianRidge,Normalizer,mean_squared_error:69.3561811831\n",
      "\n",
      "\n",
      "LinearRegression,StandardScaler,mean_squared_error:57.6335170908\n",
      "LinearRegression,Normalizer,mean_squared_error:57.6335170908\n",
      "###############################################\n",
      "\n",
      "Data:network_verIQ\n",
      "\n",
      "Accuracy of different classifier, with Normalization\n",
      "\n",
      "\n",
      "Ridge,StandardScaler,mean_squared_error:95.463298393\n",
      "Ridge,Normalizer,mean_squared_error:95.463298393\n",
      "\n",
      "\n",
      "BayesianRidge,StandardScaler,mean_squared_error:94.4286979725\n",
      "BayesianRidge,Normalizer,mean_squared_error:94.4286979725\n",
      "\n",
      "\n",
      "LinearRegression,StandardScaler,mean_squared_error:85.1482634724\n",
      "LinearRegression,Normalizer,mean_squared_error:85.1482634724\n",
      "###############################################\n",
      "\n",
      "Data:volume_language\n",
      "\n",
      "Accuracy of different classifier, with Normalization\n",
      "\n",
      "\n",
      "Ridge,StandardScaler,mean_squared_error:42.2957655762\n",
      "Ridge,Normalizer,mean_squared_error:42.2957655762\n",
      "\n",
      "\n",
      "BayesianRidge,StandardScaler,mean_squared_error:43.618849791\n",
      "BayesianRidge,Normalizer,mean_squared_error:43.618849791\n",
      "\n",
      "\n",
      "LinearRegression,StandardScaler,mean_squared_error:38.3977864772\n",
      "LinearRegression,Normalizer,mean_squared_error:38.3977864772\n",
      "###############################################\n",
      "\n",
      "Data:volume_perIQ\n",
      "\n",
      "Accuracy of different classifier, with Normalization\n",
      "\n",
      "\n",
      "Ridge,StandardScaler,mean_squared_error:71.8236742454\n",
      "Ridge,Normalizer,mean_squared_error:71.8236742454\n",
      "\n",
      "\n",
      "BayesianRidge,StandardScaler,mean_squared_error:68.8139795726\n",
      "BayesianRidge,Normalizer,mean_squared_error:68.8139795726\n",
      "\n",
      "\n",
      "LinearRegression,StandardScaler,mean_squared_error:64.5802579594\n",
      "LinearRegression,Normalizer,mean_squared_error:64.5802579594\n",
      "###############################################\n",
      "\n",
      "Data:volume_spatial\n",
      "\n",
      "Accuracy of different classifier, with Normalization\n",
      "\n",
      "\n",
      "Ridge,StandardScaler,mean_squared_error:68.9776585777\n",
      "Ridge,Normalizer,mean_squared_error:68.9776585777\n",
      "\n",
      "\n",
      "BayesianRidge,StandardScaler,mean_squared_error:69.3427559808\n",
      "BayesianRidge,Normalizer,mean_squared_error:69.3427559808\n",
      "\n",
      "\n",
      "LinearRegression,StandardScaler,mean_squared_error:62.9460126828\n",
      "LinearRegression,Normalizer,mean_squared_error:62.9460126828\n",
      "###############################################\n",
      "\n",
      "Data:volume_verIQ\n",
      "\n",
      "Accuracy of different classifier, with Normalization\n",
      "\n",
      "\n",
      "Ridge,StandardScaler,mean_squared_error:92.3866297692\n",
      "Ridge,Normalizer,mean_squared_error:92.3866297692\n",
      "\n",
      "\n",
      "BayesianRidge,StandardScaler,mean_squared_error:93.5859388658\n",
      "BayesianRidge,Normalizer,mean_squared_error:93.5859388658\n",
      "\n",
      "\n",
      "LinearRegression,StandardScaler,mean_squared_error:86.9782021808\n",
      "LinearRegression,Normalizer,mean_squared_error:86.9782021808\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_file_list = ['network_language', 'network_perIQ', 'network_spatial', \n",
    "                  'network_verIQ','volume_language','volume_perIQ',\n",
    "                  'volume_spatial', 'volume_verIQ']\n",
    "data_path = \"../data/featsele_IQ_Cog/\"\n",
    "for data_file in data_file_list:\n",
    "    print('###############################################\\n')\n",
    "    print('Data:{}\\n'.format(data_file))\n",
    "    # comma delimited is the default\n",
    "    df = pd.read_csv(os.path.join(data_path, data_file + '.csv'), header = 0)\n",
    "\n",
    "    # put the original column names in a python list\n",
    "    original_headers = list(df.columns.values)\n",
    "    feature_name_list = original_headers[1:]\n",
    "    # remove the non-numeric columns\n",
    "    df = df._get_numeric_data()\n",
    "\n",
    "    # put the numeric column names in a python list\n",
    "    numeric_headers = list(df.columns.values)\n",
    "\n",
    "    # create a numpy array with the numeric values for input into scikit-learn\n",
    "    numpy_array = np.array(df.as_matrix())\n",
    "    Y = numpy_array[:,0]\n",
    "    X = numpy_array[:,1:]\n",
    "    run_experiments(X, Y, feature_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
